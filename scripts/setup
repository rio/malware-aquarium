#!/bin/bash

set -eo pipefail

# Always execute from the repo root
script_dir=$(dirname "$0")
cd "${script_dir}/.."

DESTROY_CLUSTER='false'
CREATE_CLUSTER='false'
INSTALL_SERVICES='false'
VERBOSE='false'
PREFLIGHT_CHECK='false'
SKIP_SERVICE_DEPLOY='false'

function usage() {
    printf '
setup

Usage: setup [options]

Options:
    -h       Show this screen.
    -v       Print every line this script executes.
    -p       Execute the preflight check.
    -d       Destroy the cluster.
    -c       Create the cluster.
    -i       Install services into the cluster.
'
}

function main() {
    while getopts ":dpvcih" flag; do
        case "${flag}" in
            v) VERBOSE='true' ;;
            p) PREFLIGHT_CHECK='true' ;;
            d) DESTROY_CLUSTER='true' ;;
            c) CREATE_CLUSTER='true' ;;
            i) INSTALL_SERVICES='true' ;;
            h) usage
                exit 1
            ;;
            *) error "Unexpected flag ${OPTARG}"
                usage
                exit 1
            ;;
        esac
    done

    readonly VERBOSE
    readonly CREATE_CLUSTER
    readonly DESTROY_CLUSTER
    readonly PREFLIGHT_CHECK
    readonly INSTALL_SERVICES

    if [[ "${VERBOSE}" == 'true' ]]; then
        set -x
    fi

    test_inotify_max_user_instances

    if [[ "${PREFLIGHT_CHECK}" == 'true' ]]; then
        preflight_check
        exit
    fi

    if [[ "${DESTROY_CLUSTER}" == 'true' ]]; then
        destroy_cluster
        exit
    fi

    if [[ "${CREATE_CLUSTER}" == 'true' ]]; then
        if ! create_cluster; then
            error "failed to create k3s cluster"
            exit 1
        fi

        is_cluster_online

        if [[ "${INSTALL_SERVICES}" != 'true' ]]; then
            exit
        fi
    fi

    if [[ "${INSTALL_SERVICES}" == 'true' ]]; then
        install_services
        exit
    fi

    usage
    exit 1
}


function error() {
    local err_message
    err_message="$1"

    echo "${err_message}" >&2
    exit 1
}

function create_cluster() {
    sudo mkdir -p /etc/rancher/k3s
    sudo cp scripts/config.yaml /etc/rancher/k3s/config.yaml

    export INSTALL_K3S_CHANNEL=v1.23

    scripts/k3s-install.sh
}

function destroy_cluster() {
    k3s-uninstall.sh
}

function is_cluster_online() {
    while ! kubectl cluster-info ; do
        sleep 3;
    done

    echo " 📦 waiting for node API to be available ..."
    while [[ -z "$(kubectl get nodes -o name)" ]] ; do
        sleep 1;
    done
}

function install_services() {
    echo " 🖧 installing cilium CNI ..."
    kubectl kustomize --enable-helm kubernetes/cilium | kubectl apply -f -

    echo " 📦 waiting for nodes to report ready ..."
    kubectl wait --for condition=Ready --all nodes --timeout 300s

    echo " 📜 installing cert-manger ..."
    kubectl apply -k kubernetes/cert-manager

    echo " 🖳 installing kubevirt ..."
    kubectl apply -k kubernetes/kubevirt/controllers
    kubectl apply -k kubernetes/kubevirt/customresources

    echo " 🖳 waiting for multus to be available ..."
    kubectl wait --for condition=ready pods --selector app=multus -n kube-system --timeout 300s

    echo " 🖳 installing CNI plugins ..."
    kubectl exec -n kube-system ds/kube-multus-ds -- /usr/bin/bash -c 'cd /host/opt/cni/bin && curl -sSfL https://github.com/containernetworking/plugins/releases/download/v1.1.1/cni-plugins-linux-amd64-v1.1.1.tgz | tar xzf -'

    echo " 📜 verifying if cert-manager is online before continuing ..."
    kubectl wait --for=condition=available deployments/cert-manager-webhook --namespace cert-manager --timeout 300s
    kubectl wait --for=condition=established crds/issuers.cert-manager.io --timeout 300s
    kubectl wait --for=condition=established crds/clusterissuers.cert-manager.io --timeout 300s
    kubectl wait --for=condition=established crds/certificates.cert-manager.io --timeout 300s

    echo " 🔭 installing OpenTelemetry operator ..."
    kubectl apply -k kubernetes/opentelemetry-operator

    echo " 🔭 waiting for OpenTelemetry operator to be available ..."
    kubectl wait --for=condition=available deployments/opentelemetry-operator-controller-manager --namespace opentelemetry-operator-system --timeout 300s
    kubectl wait --for=condition=established crds/opentelemetrycollectors.opentelemetry.io --timeout 300s

    echo " 🔭 installing other monitoring systems ..."
    if [[ -z "$(kubectl get prometheus -n monitoring --no-headers --output name)" ]]; then
        kubectl kustomize --enable-helm kubernetes/monitoring | kubectl create -f -
    else
        kubectl kustomize --enable-helm kubernetes/monitoring | kubectl apply --server-side --force-conflicts -f -
    fi

    echo " 🐽 installing suricata and coredns on the malware network edge ..."
    kubectl apply -k kubernetes/suricata

    echo " 👃 installing ntopng for sniffing the malware network bridge ..."
    kubectl apply -k kubernetes/ntopng

    echo " 🕵 installing grr forensics server ..."
    kubectl apply -k kubernetes/grr

    echo " 🔘 installing big-red-button ..."
    kubectl apply -k kubernetes/big-red-button

    echo " ⬡ waiting for all jobs to complete ... "
    kubectl wait --for condition=complete --all jobs --all-namespaces --timeout 300s
    kubectl delete --all jobs --all-namespaces --wait

    echo " ⬡ waiting for cdi to be available ... "
    kubectl wait --for condition=available --all cdi --all-namespaces --timeout 300s

    echo " ⬡ waiting for kubevirt to be available ... "
    kubectl wait --for condition=available --all kubevirt --all-namespaces --timeout 300s

    echo " ⬡ waiting for prometheus to be available ... "
    kubectl wait --for condition=available --all prometheus --all-namespaces --timeout 300s

    echo " ⬡ waiting for all deployments to be available ... "
    kubectl wait --for condition=available --all deployments --all-namespaces --timeout 300s

    echo " ⬡ waiting for all pods to report ready ... "
    kubectl wait --for condition=ready --all pods --all-namespaces --timeout 300s

    echo " 🥳 all pods online "
}

function test_inotify_max_user_instances() {
    local max_user_instances

    max_user_instances="$(sysctl fs.inotify.max_user_instances --values)"

    if ((${max_user_instances} < 1024)); then
        printf "
 ⚠ Warning! Your fs.inotify.max_user_instances is set to ${max_user_instances} which is less than
 the recommended 1024. Some problems might be encountered when watching logs or
 forwarding ports.

 Run 'sysctl -w fs.inotify.max_user_instances=1024' to mitigate this and add it
 to /etc/sysctl.conf to make it persistent across reboots.
 "
    fi
}

function preflight_check() {
    echo "⬡ checking for user permissions ... "

    if ! $(groups | grep -q kvm) ; then
        printf "
 ⚠ Warning! Your user is not part of the kvm group.

 This might not allow you to use packer to build images on this machine.

 Run 'usermod -a -G kvm $(id -un)' to add your current user to the kvm group and login
 again to activate the change.

 "
    else
        printf " ✓ user $(id -un) in kvm group.
"
    fi

    echo "⬡ checking for required binaries ... "

    local kubectl_binary_path="$(command -v kubectl)"
    local helm_binary_path="$(command -v helm)"
    local packer_binary_path="$(command -v packer)"
    local qemu_img_binary_path="$(command -v qemu-img)"
    local qemu_system_x86_64_binary_path="$(command -v qemu-system-x86_64)"
    local mkisofs_binary_path="$(command -v mkisofs)"
    local virtctl_binary_path="$(command -v virtctl)"

    if [[ -z "${kubectl_binary_path}" ]]; then
        printf "
 ⚠ Warning! kubectl is missing from your path.

 kubectl is the main utility used to interact with the kubernetes API. It can be
 installed separately but it is bundled in the k3s distribution of kubernetes.
"
    else
        printf " ✓ kubectl found at ${kubectl_binary_path}.
"
    fi

    if [[ -z "${helm_binary_path}" ]]; then
        printf "
 ⚠ Warning! helm is missing from your path.

 helm is canonically known as the package manager of kubernetes. It allows templated
 kubernetes configuration to be shared, configured and installed in a convenient
 way. It can be installed from the helm installation page at https://helm.sh/docs/intro/install/.
"
    else
        printf " ✓ helm found at ${helm_binary_path}.
"
    fi

    if [[ -z "${packer_binary_path}" ]] || [[ -z "${qemu_system_x86_64_binary_path}" ]] ; then
        printf "
 ⚠ Warning! packer or qemu-system-x86_64 is missing from your path. Note this is
 only required if you are building virtual machine images. Installation of the
 infrastructure is still possible without them.

 Packer is used to build virtual machine images and it uses qemu. You can install
 it from the Packer website at https://www.packer.io/downloads. Usually installing
 qemu-system-x86 on Ubuntu or qemu-system-x86-core on Fedora is enough to satisfy
 this requirement.
"
    else
        printf " ✓ packer found at ${packer_binary_path}.
"
    fi

    if [[ -z "${qemu_img_binary_path}" ]] || [[ -z "${mkisofs_binary_path}" ]] ; then
        printf "
 ⚠ Warning! qemu-img or mkisofs is missing from your path. Note this is only required if you
 are building virtual machine images.

 These commands are used to build cd images which includes drivers required to
 install Windows. Usually they are bundled in the qemu-utils and genisoimage
 packages on Ubuntu or the qemu-img and xorriso packages on Fedora.
"
    else
        printf " ✓ qemu-img found at ${qemu_img_binary_path}.
 ✓ mkisofs found at ${mkisofs_binary_path}.
"
    fi

    if [[ -z "${virtctl_binary_path}" ]]; then
        printf "
 ⚠ Warning! virtctl is missing from your path. It is not required to get the
 infrastructure running but it is highly recommended.

 virtctl is a utility that wraps around the KubeVirt API to make working with it
 more convenient. It can ssh into virtual machines, start VNC sessions and upload
 virtual machine disk images to the cluster. The upload-image is the only one used
 by our sync-images script. You can find installation instructions on the KubeVirt
 documentation website http://kubevirt.io/user-guide/operations/virtctl_client_tool/.
"
    else
        printf " ✓ virtctl found at ${virtctl_binary_path}.
"
    fi
}

main "$@"