#!/bin/bash

set -eo pipefail

# Always execute from the repo root
script_dir=$(dirname "$0")
cd "${script_dir}/.."

declare -xr KIND_CLUSTER_NAME='malware-aquarium'

DESTROY='false'
VERBOSE='false'
PREFLIGHT_CHECK='false'
SKIP_SERVICE_DEPLOY='false'

function main() {
    while getopts ":dpvsh" flag; do
        case "${flag}" in
            v) VERBOSE='true' ;;
            p) PREFLIGHT_CHECK='true' ;;
            d) DESTROY='true' ;;
            s) SKIP_SERVICE_DEPLOY='true' ;;
            h) usage
                exit 1
            ;;
            *) error "Unexpected flag ${OPTARG}"
                usage
                exit 1
            ;;
        esac
    done

    readonly VERBOSE
    readonly DESTROY
    readonly PREFLIGHT_CHECK
    readonly SKIP_SKAFFOLD_DEPLOY

    if [[ "${VERBOSE}" == 'true' ]]; then
        set -x
    fi

    test_inotify_max_user_instances

    if [[ "${PREFLIGHT_CHECK}" == 'true' ]]; then
        preflight_check
        exit
    fi

    if [[ "${DESTROY}" == 'true' ]]; then
        destroy_cluster
        exit
    fi

    if ! create_cluster; then
        error "failed to create kind cluster"
        exit 1
    fi

    if [[ "${SKIP_SERVICE_DEPLOY}" != 'true' ]]; then
        install_services
    fi
}

function usage() {
    printf '
setup

Usage: setup [options]

Options:
    -h       Show this screen.
    -v       Print every line this script executes.
    -d       Destroy the cluster.
    -s       Skip deploying services.
    -p       Execute the preflight check.
'
}

function error() {
    local err_message
    err_message="$1"

    echo "${err_message}" >&2
    exit 1
}

function install_cni_plugins() {
    local kind_node
    kind_node="$(kind get nodes | head -n 1)"
    if (( $? != 0 )); then
        error "failed to get cluster nodes"
        exit 1
    fi

    docker exec --workdir /opt/cni/bin "${kind_node}" /bin/bash -c 'curl -sSfL https://github.com/containernetworking/plugins/releases/download/v1.1.1/cni-plugins-linux-amd64-v1.1.1.tgz | tar xzf -'
}

function create_cluster() {
    if ! cluster_exists; then
        kind create cluster --config scripts/config.yaml
        install_cni_plugins
    fi
}

function destroy_cluster() {
    kind delete cluster
}

function cluster_exists() {
    if [[ "$(kind get clusters --quiet)" != "${KIND_CLUSTER_NAME}" ]]; then
        return 1
    fi

    return 0
}

function install_services() {
    echo " ğŸ–§ installing cilium CNI ..."
    kubectl kustomize --enable-helm kubernetes/cilium | kubectl apply -f -

    echo " ğŸ“¦ waiting for nodes to report ready ..."
    kubectl wait --for condition=Ready --all nodes --timeout 300s

    echo " ğŸ“œ installing cert-manger ..."
    kubectl apply -k kubernetes/cert-manager

    echo " ğŸ–³ installing kubevirt ..."
    kubectl apply -k kubernetes/kubevirt/controllers
    kubectl apply -k kubernetes/kubevirt/customresources

    echo " ğŸ–³ waiting for multus to be available ..."
    kubectl wait --for condition=ready pods --selector app=multus -n kube-system --timeout 300s

    echo " ğŸ“œ verifying if cert-manager is online before continuing ..."
    kubectl wait --for=condition=available deployments/cert-manager-webhook --namespace cert-manager --timeout 300s
    kubectl wait --for=condition=established crds/issuers.cert-manager.io --timeout 300s
    kubectl wait --for=condition=established crds/clusterissuers.cert-manager.io --timeout 300s
    kubectl wait --for=condition=established crds/certificates.cert-manager.io --timeout 300s

    echo " ğŸ”­ installing OpenTelemetry operator ..."
    kubectl apply -k kubernetes/opentelemetry-operator

    echo " ğŸ”­ waiting for OpenTelemetry operator to be available ..."
    kubectl wait --for=condition=available deployments/opentelemetry-operator-controller-manager --namespace opentelemetry-operator-system --timeout 300s
    kubectl wait --for=condition=established crds/opentelemetrycollectors.opentelemetry.io --timeout 300s

    echo " ğŸ”­ installing other monitoring systems ..."
    if [[ -z "$(kubectl get prometheus -n monitoring --no-headers --output name)" ]]; then
        kubectl kustomize --enable-helm kubernetes/monitoring | kubectl create -f -
    else
        kubectl kustomize --enable-helm kubernetes/monitoring | kubectl apply --server-side --force-conflicts -f -
    fi

    echo " ğŸ½ installing suricata and coredns on the malware network edge ..."
    kubectl apply -k kubernetes/suricata

    echo " ğŸ‘ƒ installing ntopng for sniffing the malware network bridge ..."
    kubectl apply -k kubernetes/ntopng

    echo " ğŸ•µ installing grr forensics server ..."
    kubectl apply -k kubernetes/grr

    echo " ğŸ”˜ installing big-red-button ..."
    kubectl apply -k kubernetes/big-red-button

    echo " â¬¡ waiting for all jobs to complete ... "
    kubectl wait --for condition=complete --all jobs --all-namespaces --timeout 300s
    kubectl delete --all jobs --all-namespaces --wait

    echo " â¬¡ waiting for cdi to be available ... "
    kubectl wait --for condition=available --all cdi --all-namespaces --timeout 300s

    echo " â¬¡ waiting for kubevirt to be available ... "
    kubectl wait --for condition=available --all kubevirt --all-namespaces --timeout 300s

    echo " â¬¡ waiting for prometheus to be available ... "
    kubectl wait --for condition=available --all prometheus --all-namespaces --timeout 300s

    echo " â¬¡ waiting for all deployments to be available ... "
    kubectl wait --for condition=available --all deployments --all-namespaces --timeout 300s

    echo " â¬¡ waiting for all pods to report ready ... "
    kubectl wait --for condition=ready --all pods --all-namespaces --timeout 300s

    echo " ğŸ¥³ all pods online "
}

function test_inotify_max_user_instances() {
    local max_user_instances

    max_user_instances="$(sysctl fs.inotify.max_user_instances --values)"

    if ((${max_user_instances} < 1024)); then
        printf "
 âš  Warning! Your fs.inotify.max_user_instances is set to ${max_user_instances} which is less than
 the recommended 1024. Some problems might be encountered when watching logs or
 forwarding ports.

 Run 'sysctl -w fs.inotify.max_user_instances=1024' to mitigate this and add it
 to /etc/sysctl.conf to make it persistent across reboots.
 "
    fi
}

function preflight_check() {
    echo " â¬¡ checking for required binaries ... "

    local kubectl_binary_path="$(command -v kubectl)"
    local helm_binary_path="$(command -v helm)"
    local packer_binary_path="$(command -v packer)"
    local qemu_img_binary_path="$(command -v qemu-img)"
    local mkisofs_binary_path="$(command -v mkisofs)"
    local virtctl_binary_path="$(command -v virtctl)"

    if [[ -z "${kubectl_binary_path}" ]]; then
        printf "
 âš  Warning! kubectl is missing from your path.

 kubectl is the main utility used to interact with the kubernetes API. It can be
 installed separately but it is bundled in the k3s distribution of kubernetes.
"
    else
        printf "
 âœ“ kubectl found at ${kubectl_binary_path}.
        "
    fi

    if [[ -z "${helm_binary_path}" ]]; then
        printf "
 âš  Warning! helm is missing from your path.

 helm is canonically known as the package manager of kubernetes. It allows templated
 kubernetes configuration to be shared, configured and installed in a convenient
 way. It can be installed from the helm installation page at https://helm.sh/docs/intro/install/.
"
    else
        printf "
 âœ“ helm found at ${helm_binary_path}.
        "
    fi

    if [[ -z "${packer_binary_path}" ]]; then
        printf "
 âš  Warning! packer is missing from your path. Note this is only required if you
 are building virtual machine images

 Packer is used to build virtual machine images. You can install it from the
 Packer website at https://www.packer.io/downloads.
"
    else
        printf "
 âœ“ packer found at ${packer_binary_path}.
        "
    fi

    if [[ -z "${qemu_img_binary_path}" ]] || [[ -z "${mkisofs_binary_path}" ]] ; then
        printf "
 âš  Warning! qemu-img or mkisofs is missing from your path. Note this is only required if you
 are building virtual machine images.

 These commands are used to build cd images which includes drivers required to
 install Windows. Usually they are bundled in the qemu-utils and genisoimage
 packages on Ubuntu or the qemu-img and xorriso packages on Fedora.
"
    else
        printf "
 âœ“ qemu-img found at ${qemu_img_binary_path}.

 âœ“ mkisofs found at ${mkisofs_binary_path}.
        "
    fi

    if [[ -z "${virtctl_binary_path}" ]]; then
        printf "
 âš  Warning! virtctl is missing from your path. It is not required to get the
 infrastructure running but it is highly recommended.

 virtctl is a utility that wraps around the KubeVirt API to make working with it
 more convenient. It can ssh into virtual machines, start VNC sessions and upload
 virtual machine disk images to the cluster. The upload-image is the only one used
 by our sync-images script. You can find installation instructions on the KubeVirt
 documentation website http://kubevirt.io/user-guide/operations/virtctl_client_tool/.
"
    else
        printf "
 âœ“ virtctl found at ${virtctl_binary_path}.
        "
    fi
}

main "$@"