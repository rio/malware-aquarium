#!/bin/bash

set -eo pipefail

# Always execute from the repo root
script_dir=$(dirname "$0")
cd "${script_dir}/.."

declare -xr KIND_CLUSTER_NAME='malware-aquarium'

DESTROY='false'
VERBOSE='false'
PREFLIGHT_CHECK='false'
SKIP_SERVICE_DEPLOY='false'

function main() {
    while getopts ":dpvsh" flag; do
        case "${flag}" in
            v) VERBOSE='true' ;;
            p) PREFLIGHT_CHECK='true' ;;
            d) DESTROY='true' ;;
            s) SKIP_SERVICE_DEPLOY='true' ;;
            h) usage
                exit 1
            ;;
            *) error "Unexpected flag ${OPTARG}"
                usage
                exit 1
            ;;
        esac
    done

    readonly VERBOSE
    readonly DESTROY
    readonly PREFLIGHT_CHECK
    readonly SKIP_SKAFFOLD_DEPLOY

    if [[ "${VERBOSE}" == 'true' ]]; then
        set -x
    fi

    test_inotify_max_user_instances

    if [[ "${PREFLIGHT_CHECK}" == 'true' ]]; then
        preflight_check
        exit
    fi

    if [[ "${DESTROY}" == 'true' ]]; then
        destroy_cluster
        exit
    fi

    if ! create_cluster; then
        error "failed to create kind cluster"
        exit 1
    fi

    if [[ "${SKIP_SERVICE_DEPLOY}" != 'true' ]]; then
        install_services
    fi
}

function usage() {
    printf '
setup

Usage: setup [options]

Options:
    -h       Show this screen.
    -v       Print every line this script executes.
    -d       Destroy the cluster.
    -s       Skip deploying services.
    -p       Execute the preflight check.
'
}

function error() {
    local err_message
    err_message="$1"

    echo "${err_message}" >&2
    exit 1
}

function install_cni_plugins() {
    local kind_node
    kind_node="$(kind get nodes | head -n 1)"
    if (( $? != 0 )); then
        error "failed to get cluster nodes"
        exit 1
    fi

    docker exec --workdir /opt/cni/bin "${kind_node}" /bin/bash -c 'curl -sSfL https://github.com/containernetworking/plugins/releases/download/v1.1.1/cni-plugins-linux-amd64-v1.1.1.tgz | tar xzf -'
}

function create_cluster() {
    if ! cluster_exists; then
        kind create cluster --config scripts/config.yaml
        install_cni_plugins
    fi
}

function destroy_cluster() {
    kind delete cluster
}

function cluster_exists() {
    if [[ "$(kind get clusters --quiet)" != "${KIND_CLUSTER_NAME}" ]]; then
        return 1
    fi

    return 0
}

function install_services() {
    echo " ğŸ–§ installing cilium CNI ..."
    kubectl kustomize --enable-helm kubernetes/cilium | kubectl apply -f -

    echo " ğŸ“¦ waiting for nodes to report ready ..."
    kubectl wait --for condition=Ready --all nodes --timeout 300s

    echo " ğŸ“œ installing cert-manger ..."
    kubectl apply -k kubernetes/cert-manager

    echo " ğŸ–³ installing kubevirt ..."
    kubectl apply -k kubernetes/kubevirt/controllers
    kubectl apply -k kubernetes/kubevirt/customresources

    echo " ğŸ–³ waiting for multus to be available ..."
    kubectl wait --for condition=ready pods --selector app=multus -n kube-system --timeout 300s

    echo " ğŸ“œ verifying if cert-manager is online before continuing ..."
    kubectl wait --for=condition=available deployments/cert-manager-webhook --namespace cert-manager --timeout 300s
    kubectl wait --for=condition=established crds/issuers.cert-manager.io --timeout 300s
    kubectl wait --for=condition=established crds/clusterissuers.cert-manager.io --timeout 300s
    kubectl wait --for=condition=established crds/certificates.cert-manager.io --timeout 300s

    echo " ğŸ”­ installing OpenTelemetry operator ..."
    kubectl apply -k kubernetes/opentelemetry-operator

    echo " ğŸ”­ waiting for OpenTelemetry operator to be available ..."
    kubectl wait --for=condition=available deployments/opentelemetry-operator-controller-manager --namespace opentelemetry-operator-system --timeout 300s
    kubectl wait --for=condition=established crds/opentelemetrycollectors.opentelemetry.io --timeout 300s

    echo " ğŸ”­ installing other monitoring systems ..."
    if [[ -z "$(kubectl get prometheus -n monitoring --no-headers --output name)" ]]; then
        kubectl kustomize --enable-helm kubernetes/monitoring | kubectl create -f -
    else
        kubectl kustomize --enable-helm kubernetes/monitoring | kubectl apply --server-side --force-conflicts -f -
    fi

    echo " ğŸ½ installing suricata and coredns on the malware network edge ..."
    kubectl apply -k kubernetes/suricata

    echo " ğŸ‘ƒ installing ntopng for sniffing the malware network bridge ..."
    kubectl apply -k kubernetes/ntopng

    echo " ğŸ•µ installing grr forensics server ..."
    kubectl apply -k kubernetes/grr

    echo " ğŸ”˜ installing big-red-button ..."
    kubectl apply -k kubernetes/big-red-button
    
    echo " â¬¡ waiting for all jobs to complete ... "
    kubectl wait --for condition=complete --all jobs --all-namespaces --timeout 300s
    kubectl delete --all jobs --all-namespaces --wait

    echo " â¬¡ waiting for cdi to be available ... "
    kubectl wait --for condition=available --all cdi --all-namespaces --timeout 300s

    echo " â¬¡ waiting for kubevirt to be available ... "
    kubectl wait --for condition=available --all kubevirt --all-namespaces --timeout 300s

    echo " â¬¡ waiting for prometheus to be available ... "
    kubectl wait --for condition=available --all prometheus --all-namespaces --timeout 300s

    echo " â¬¡ waiting for all deployments to be available ... "
    kubectl wait --for condition=available --all deployments --all-namespaces --timeout 300s

    echo " â¬¡ waiting for all pods to report ready ... "
    kubectl wait --for condition=ready --all pods --all-namespaces --timeout 300s

    echo " ğŸ¥³ all pods online "
}

function test_inotify_max_user_instances() {
    local max_user_instances

    max_user_instances="$(sysctl fs.inotify.max_user_instances --values)"

    if ((${max_user_instances} < 1024)); then
        printf "
 âš  Warning! Your fs.inotify.max_user_instances is set to ${max_user_instances}
 which is less than the recommended 1024. Some problems might be encountered
 when watching logs or forwarding ports.
 
 Run 'sysctl -w fs.inotify.max_user_instances=1024' to mitigate this.
 "
    fi
}

main "$@"